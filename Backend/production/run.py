"""
Complete End-to-End Text Optimization Framework

Framework Goal:
1. Take a prompt as input
2. Auto-classify prompt category using NLP patterns
3. Apply RL-based token reduction (30%+ with 80%+ semantic meaning)
4. Select suitable LLM based on auto-detected category
5. Generate response from selected LLM

Usage: python run.py [prompt]
"""

import os
import sys
import subprocess
from stable_baselines3 import PPO
from rl_optimizer import RLOptimizer, get_latest_training_data_file, DEFAULT_TRAINING_TIMESTEPS
from prompt_diversity_test import PromptDiversityTester

def process_prompt(prompt):
    """
    Complete framework pipeline: Input â†’ RL Token Reduction â†’ LLM Selection â†’ Response Generation
    
    Args:
        prompt (str): Input prompt to process
        
    Returns:
        dict: Complete results including optimized prompt, selected LLM, and response
    """
    print("=" * 80)
    print("ğŸš€ FRAMEWORK: TOKEN REDUCTION & LLM RESPONSE GENERATION")
    print("=" * 80)
    print(f"ğŸ“ Input Prompt: {prompt}")
    print("-" * 80)

    try:
        training_data_file = get_latest_training_data_file("./results")
        rl_optimizer = RLOptimizer(training_data_file)
        
        prompt_tester = PromptDiversityTester()
        
        category = prompt_tester.classify_prompt(prompt)
        print(f"ğŸ“‚ Auto-detected Category: {category}")
        
        model_path = "./models/text_optimizer_ppo.zip"
        if os.path.exists(model_path):
            print(f"ğŸ“¦ Loading trained RL model: {model_path}")
            rl_optimizer.model = PPO.load(model_path)
        else:
            print("âš ï¸ No trained model found. Training new model...")
            rl_optimizer.train(total_timesteps=DEFAULT_TRAINING_TIMESTEPS, save_path=model_path)
        
        print(f"ğŸ¯ Processing prompt: {prompt}...")
        action, strategy = rl_optimizer.predict_optimal_strategy(prompt, category)
        optimized_prompt, metrics = rl_optimizer.env.apply_optimization_strategy(prompt, action)
        
        print(f"ğŸ“Š Token Reduction: {metrics['reduction_percent']:.1f}%")
        print(f"ğŸ“Š Similarity: {metrics['similarity']:.3f}")
        print(f"ğŸ“Š Strategy Used: {strategy}")
        
        if metrics['reduction_percent'] < 25:
            print("âš ï¸  Warning: Token reduction below 25% target")
        if metrics['similarity'] < 0.8:
            print("âš ï¸  Warning: Similarity below 80% target")
            
        selected_llm = prompt_tester.route_prompt_to_llm(optimized_prompt)
        print(f"ğŸ¤– Selected LLM: {selected_llm}")
        
        try:
            response = subprocess.check_output(
                ["ollama", "run", selected_llm, optimized_prompt], 
                text=True, timeout=120
            ).strip()
            
            result = {
                'original_prompt': prompt,
                'optimized_prompt': optimized_prompt,
                'selected_llm': selected_llm,
                'response': response,
                'metrics': metrics,
                'strategy_used': strategy
            }
            
        except Exception as e:
            print(f"âŒ LLM generation failed: {e}")
            result = {
                'original_prompt': prompt,
                'optimized_prompt': optimized_prompt,
                'selected_llm': selected_llm,
                'response': f"Error generating response: {e}",
                'metrics': metrics,
                'strategy_used': strategy
            }
        print("\n" + "=" * 80)
        print("âœ… FRAMEWORK RESULTS:")
        print("=" * 80)
        print(f"ğŸ¯ Optimized Prompt: {result['optimized_prompt']}")
        print(f"ğŸ¤– Selected LLM: {result['selected_llm']}")
        print(f"ğŸ“Š Strategy: {result['strategy_used']}")
        print(f"ğŸ“ˆ Token Reduction: {result['metrics']['reduction_percent']:.1f}%")
        print(f"ğŸ­ Similarity: {result['metrics']['similarity']:.3f}")
        print(f"âœ… Target Met: {'Yes' if result['metrics']['target_achieved'] else 'No'}")
        print("-" * 80)
        print("ğŸ¤– LLM RESPONSE:")
        print(result['response'])
        print("=" * 80)
        
        return result
        
    except Exception as e:
        print(f"âŒ Framework failed: {e}")
        import traceback
        traceback.print_exc()
        return None

def main():
    """Main entry point for the framework."""
    if len(sys.argv) >= 2:
        prompt = sys.argv[1]
        process_prompt(prompt)
    else:
        print("ğŸ¯ INTERACTIVE MODE - Testing Framework with Sample Prompts")
        print("=" * 80)
        
        sample_prompts = [
            "Calculate the area under the curve y = x^2 from x = 0 to x = 4 using integration",
            "Write a Python function to implement quicksort algorithm with error handling",
            "Design a marketing strategy for launching a new smartphone targeting millennials"
        ]
        
        for i, prompt in enumerate(sample_prompts, 1):
            print(f"\nğŸ”„ TESTING PROMPT {i}/3:")
            result = process_prompt(prompt)
            
            if result:
                print(f"âœ… Test {i} completed successfully!")
            else:
                print(f"âŒ Test {i} failed!")
            
            print("\n" + "="*80)

if __name__ == "__main__":
    main()
